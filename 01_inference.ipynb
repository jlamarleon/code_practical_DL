{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870610e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸš€ Install required packages for running the model\n",
    "# - transformers: for loading and running LLMs\n",
    "# - datasets: optional, for dataset handling\n",
    "# - peft: for later LoRA experiments\n",
    "# - accelerate: for optimized GPU usage in Colab\n",
    "!pip install -q transformers datasets peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263c4f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ” Check GPU availability in Colab\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU not available. Make sure Runtime -> Change runtime type -> GPU is selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0339892",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ” Check GPU properties and memory in Colab\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"âœ… GPU is available: {torch.cuda.get_device_name(device)}\")\n",
    "    print(f\"  - CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  - PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # GPU memory stats\n",
    "    total_mem = torch.cuda.get_device_properties(device).total_memory / 1e9\n",
    "    reserved_mem = torch.cuda.memory_reserved(device) / 1e9\n",
    "    allocated_mem = torch.cuda.memory_allocated(device) / 1e9\n",
    "    free_mem = reserved_mem - allocated_mem\n",
    "    \n",
    "    print(f\"  - Total memory: {total_mem:.2f} GB\")\n",
    "    print(f\"  - Reserved memory: {reserved_mem:.2f} GB\")\n",
    "    print(f\"  - Allocated memory: {allocated_mem:.2f} GB\")\n",
    "    print(f\"  - Free memory in reserved pool: {free_mem:.2f} GB\")\n",
    "else:\n",
    "    print(\"âŒ GPU not available. Make sure Runtime -> Change runtime type -> GPU is selected\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
