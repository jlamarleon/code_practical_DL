# ğŸ“š Deep Learning Practical: Transformers & LLMs

Welcome to the **Deep Learning Practical** focused on **Transformers and Large Language Models (LLMs)**.  
This course will guide you **from understanding the Transformer architecture to running inference and fine-tuning models** like Qwen and TinyLlama.

---

## ğŸ¯ Learning Objectives

By the end of this practical session, students will be able to:

1. Understand the **core Transformer architecture** (self-attention, MLP, residuals, layer norms).  
2. Load a **pre-trained 1B+ parameter LLM** and run **text generation**.  
3. Explore **internal Transformer components** to understand weights, projections, and layers.  
4. Perform **first experiments in model fine-tuning** using full parameter updates.  
5. Apply **LoRA (Low-Rank Adaptation)** for **parameter-efficient fine-tuning**.  
6. Use professional workflow with **VS Code, GitHub, and Google Colab**.  

---

## ğŸ“‚ Repository Structure
```text
DL_LLM_Practical/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_inference.ipynb         # Load model, run prompts, explore Transformer internals
â”‚   â”œâ”€â”€ 02_full_finetuning.ipynb   # Full fine-tuning on small dataset
â”‚   â”œâ”€â”€ 03_lora_finetuning.ipynb   # LoRA fine-tuning experiments
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ small_dataset.json          # Sample instruction dataset
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies for the notebooks
â””â”€â”€ README.md


---

## âš¡ Getting Started

### 1ï¸âƒ£ Open Notebook in Google Colab

Click the badge to launch the notebook directly:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USER/DL_LLM_Practical/blob/main/notebooks/01_inference.ipynb)

Or manually:

1. Go to [Google Colab](https://colab.research.google.com)  
2. Click **File â†’ Open notebook â†’ GitHub**  
3. Paste the repository URL:  


4. Select `01_inference.ipynb`

---

### 2ï¸âƒ£ Set Runtime Type

To use GPU:

- **Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU**  

This ensures the LLM runs efficiently.

---

### 3ï¸âƒ£ Run the First Cell

The first cell installs all required packages:

```python
# ğŸš€ Install required packages for running the model
# - transformers: for loading and running LLMs
# - datasets: optional, for dataset handling
# - peft: for later LoRA experiments
# - accelerate: for optimized GPU usage in Colab
!pip install -q transformers datasets peft accelerate